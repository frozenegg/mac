{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3+1chase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbYLNSRDjDn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment:\n",
        "    class Hunter:\n",
        "        def __init__(self, height, width):\n",
        "            self.height = height\n",
        "            self.width = width\n",
        "\n",
        "        def initialize_state(self):\n",
        "            state = np.random.rand(2)\n",
        "            environment = np.array([self.height, self.width])\n",
        "            state = state * environment\n",
        "            return state\n",
        "        \n",
        "    class Prey:\n",
        "        def __init__(self, height, width):\n",
        "            self.height = height\n",
        "            self.width = width\n",
        "\n",
        "        def initialize_state(self):\n",
        "            state = np.random.rand(2)\n",
        "            environment = np.array([self.height, self.width])\n",
        "            state = state * environment\n",
        "            return state\n",
        "\n",
        "        def move_away_from_hunters(self, prey_state, hunter_state1, hunter_state2, hunter_state3):\n",
        "            hunters_g = (hunter_state1 + hunter_state2 + hunter_state3) / 3\n",
        "            g_to_prey_state_vector = prey_state - hunters_g\n",
        "            g_to_prey_state_vector_norm = g_to_prey_state_vector / np.linalg.norm(g_to_prey_state_vector)\n",
        "            new_prey_state = prey_state + g_to_prey_state_vector_norm * 0.5\n",
        "            return new_prey_state\n",
        "    \n",
        "    def __init__(self):\n",
        "        # self.done = False\n",
        "        self.max_episode_steps = 10\n",
        "        self.reward1 = 1\n",
        "        self.reward2 = -0.1\n",
        "        self.reward3 = -0.5\n",
        "        self.area_threshold = 0.1\n",
        "        self.prey = self.Prey(1,1)\n",
        "        self.hunter1 = self.Hunter(1,1)\n",
        "        self.hunter2 = self.Hunter(1,1)\n",
        "        self.hunter3 = self.Hunter(1,1)\n",
        "        \n",
        "        self.hunter_state1 = None\n",
        "        self.hunter_state2 = None\n",
        "        self.hunter_state3 = None\n",
        "        self.prey_state = None\n",
        "        \n",
        "        self.initialize_state()\n",
        "        \n",
        "    def initialize_state(self):\n",
        "        self.hunter_state1 = self.hunter1.initialize_state()\n",
        "        self.hunter_state2 = self.hunter2.initialize_state()\n",
        "        self.hunter_state3 = self.hunter3.initialize_state()\n",
        "        self.prey_state = self.prey.initialize_state()\n",
        "        \n",
        "        return self.hunter_state1, self.hunter_state2, self.hunter_state3, self.prey_state\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return np.exp(x) / (np.exp(x) + 1)\n",
        "    \n",
        "    def step(self, action1, action2, action3):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        \n",
        "        self.prey_state = self.prey.move_away_from_hunters(self.prey_state, self.hunter_state1, self.hunter_state2, self.hunter_state3)\n",
        "        self.hunter_state1 = self.hunter_state1 + np.array([self.sigmoid(action1[0]) * np.cos(np.pi * self.sigmoid(action1[1]))])\n",
        "        self.hunter_state2 = self.hunter_state2 + np.array([self.sigmoid(action2[0]) * np.cos(np.pi * self.sigmoid(action2[1]))])\n",
        "        self.hunter_state3 = self.hunter_state3 + np.array([self.sigmoid(action3[0]) * np.cos(np.pi * self.sigmoid(action3[1]))])\n",
        "        \n",
        "        hunter2_vector_from_hunter1 = self.hunter_state2 - self.hunter_state1\n",
        "        hunter3_vector_from_hunter1 = self.hunter_state3 - self.hunter_state1\n",
        "        prey_vector_from_hunter1 = self.prey_state - self.hunter_state1\n",
        "        \n",
        "        cross_h1_h2_p = np.cross(hunter2_vector_from_hunter1, prey_vector_from_hunter1)\n",
        "        cross_h1_h2_h3 = np.cross(hunter2_vector_from_hunter1, hunter3_vector_from_hunter1)\n",
        "\n",
        "        area_h1_h2_p = np.linalg.norm(cross_h1_h2_p)\n",
        "        area_h1_h2_h3 = np.linalg.norm(cross_h1_h2_h3)\n",
        "\n",
        "        cross_h1_h2_p_sign = np.sign(cross_h1_h2_p)\n",
        "        cross_h1_h2_h3_sign = np.sign(cross_h1_h2_h3)\n",
        "\n",
        "        cos_h1_h2_p = np.dot(hunter2_vector_from_hunter1, prey_vector_from_hunter1) / (np.linalg.norm(hunter2_vector_from_hunter1) * np.linalg.norm(prey_vector_from_hunter1))\n",
        "        cos_h1_h2_h3 = np.dot(hunter2_vector_from_hunter1, hunter3_vector_from_hunter1) / (np.linalg.norm(hunter2_vector_from_hunter1) * np.linalg.norm(hunter3_vector_from_hunter1))\n",
        "        \n",
        "        if((area_h1_h2_p < area_h1_h2_h3) and (cross_h1_h2_p_sign == cross_h1_h2_h3_sign) and (cos_h1_h2_p > cos_h1_h2_h3)):\n",
        "            if(area_h1_h2_h3 < self.area_threshold):\n",
        "              reward = self.reward1\n",
        "              done = True\n",
        "            else:\n",
        "              reward = self.reward2            \n",
        "        else:\n",
        "            reward = self.reward3\n",
        "        \n",
        "        return self.prey_state, self.hunter_state1, self.hunter_state2, self.hunter_state3, reward, done"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRDqC3zWjERz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "def reparameterize(means, log_stds):\n",
        "    stds = log_stds.exp()\n",
        "    noises = torch.randn_like(means)\n",
        "    us = means + noises * stds\n",
        "    actions = torch.tanh(us)\n",
        "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
        "    return actions, log_pis\n",
        "\n",
        "def calculate_log_pi(log_stds, noises, actions):\n",
        "    gaussian_log_probs = (-0.5 * noises.pow(2) - log_stds).sum(dim=-1, keepdim=True) - 0.5* math.log(2 * math.pi) * log_stds.size(-1)\n",
        "    log_pis = gaussian_log_probs - torch.log(1 - actions.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
        "    return log_pis\n",
        "\n",
        "class SACActor(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(state_shape[0],256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256,256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 2*action_shape[0])\n",
        "        )\n",
        "        \n",
        "    def forward(self, states):\n",
        "        return torch.tanh(self.net(states).chunk(2,dim=-1)[0])\n",
        "    \n",
        "    def sample(self, states):\n",
        "        means, log_stds = self.net(states).chunk(2,dim=-1)\n",
        "        return reparameterize(means, log_stds.clamp_(-20,2))\n",
        "    \n",
        "class SACCritic(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net1 = nn.Sequential(\n",
        "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256,256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256,1)\n",
        "        )\n",
        "        \n",
        "        self.net2 = nn.Sequential(\n",
        "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256,256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256,1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, states, actions):\n",
        "        x = torch.cat([states, actions], dim=-1)\n",
        "        return self.net1(x), self.net2(x)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvWayHEDjG_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
        "        self._p = 0\n",
        "        self._n = 0\n",
        "        self.buffer_size = buffer_size\n",
        "        \n",
        "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
        "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
        "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
        "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
        "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
        "        \n",
        "    def append(self, state, action, reward, done, next_state):\n",
        "        self.states[self._p].copy_(torch.from_numpy(state))\n",
        "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
        "        self.rewards[self._p] = float(reward)\n",
        "        self.dones[self._p] = float(done)\n",
        "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
        "        \n",
        "        self._p = (self._p + 1) % self.buffer_size\n",
        "        self._n = min(self._n + 1, self.buffer_size)\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
        "        return(\n",
        "            self.states[idxes],\n",
        "            self.actions[idxes],\n",
        "            self.rewards[idxes],\n",
        "            self.dones[idxes],\n",
        "            self.next_states[idxes]\n",
        "        )"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XbQWFi4jKP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Algorithm(ABC):\n",
        "    def explore(self, state, actor):\n",
        "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
        "        with torch.no_grad():\n",
        "            action, log_pi = actor.sample(state)\n",
        "        return action.cpu().numpy()[0], log_pi.item()\n",
        "    \n",
        "    def exploit(self, state, actor):\n",
        "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
        "        with torch.no_grad():\n",
        "            action = actor(state)\n",
        "        return action.cpu().numpy()[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zKFGE_3jMDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math, torch\n",
        "import numpy as np\n",
        "\n",
        "env = Environment()\n",
        "\n",
        "class SAC(Algorithm):\n",
        "    def __init__(self, state_shape, action_shape, device=torch.device('cuda'), seed=0, batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4, replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        \n",
        "        self.buffer1 = ReplayBuffer(\n",
        "            buffer_size=replay_size,\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        self.buffer2 = ReplayBuffer(\n",
        "            buffer_size=replay_size,\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        self.buffer3 = ReplayBuffer(\n",
        "            buffer_size=replay_size,\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        self.actor1 = SACActor(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic1 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic_target1 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device).eval()\n",
        "        \n",
        "        self.actor2 = SACActor(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic2 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic_target2 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device).eval()\n",
        "        \n",
        "        self.actor3 = SACActor(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic3 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device)\n",
        "        \n",
        "        self.critic_target3 = SACCritic(\n",
        "            state_shape=state_shape,\n",
        "            action_shape=action_shape\n",
        "        ).to(device).eval()\n",
        "        \n",
        "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
        "        for param in self.critic_target1.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
        "        for param in self.critic_target2.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        self.critic_target3.load_state_dict(self.critic3.state_dict())\n",
        "        for param in self.critic_target3.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.optim_actor1 = torch.optim.Adam(self.actor1.parameters(), lr=lr_actor)\n",
        "        self.optim_actor2 = torch.optim.Adam(self.actor1.parameters(), lr=lr_actor)\n",
        "        self.optim_actor3 = torch.optim.Adam(self.actor1.parameters(), lr=lr_actor)\n",
        "        \n",
        "        self.optim_critic1 = torch.optim.Adam(self.critic1.parameters(), lr=lr_critic)\n",
        "        self.optim_critic2 = torch.optim.Adam(self.critic2.parameters(), lr=lr_critic)\n",
        "        self.optim_critic3 = torch.optim.Adam(self.critic3.parameters(), lr=lr_critic)\n",
        "        \n",
        "        self.learning_steps = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.start_steps = start_steps\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "        self.reward_scale = reward_scale\n",
        "        \n",
        "    def is_update(self, steps):\n",
        "        return steps >= max(self.start_steps, self.batch_size)\n",
        "    \n",
        "    def step(self, env, hunter_state1, hunter_state2, hunter_state3, t, steps):\n",
        "        t += 1\n",
        "        \n",
        "        if steps <= self.start_steps:\n",
        "            r = np.random.rand(3)\n",
        "            theta = 2 * np.pi * np.random.rand(3)\n",
        "            action = np.array([r * np.cos(theta), r * np.sin(theta)])\n",
        "            action = action.T\n",
        "            action_1 = action[0] \n",
        "            action_2 = action[1] \n",
        "            action_3 = action[2] \n",
        "        else:\n",
        "            action_1,_ = self.explore(hunter_state1, self.actor1)\n",
        "            action_2,_ = self.explore(hunter_state2, self.actor2)\n",
        "            action_3,_ = self.explore(hunter_state3, self.actor3)\n",
        "        \n",
        "        next_prey_state, next_hunter_state1, next_hunter_state2, next_hunter_state3, reward, done = env.step(action_1, action_2, action_3)\n",
        "        \n",
        "        if t == env.max_episode_steps:\n",
        "            done_masked = False\n",
        "        else:\n",
        "            done_masked = done\n",
        "            \n",
        "        self.buffer1.append(hunter_state1, action_1, reward, done_masked, next_hunter_state1)\n",
        "        self.buffer2.append(hunter_state2, action_2, reward, done_masked, next_hunter_state2)\n",
        "        self.buffer3.append(hunter_state3, action_3, reward, done_masked, next_hunter_state3)\n",
        "        \n",
        "        if done:\n",
        "            t = 0\n",
        "            next_hunter_state1, next_hunter_state2, next_hunter_state3, next_prey_state = env.initialize_state()\n",
        "        \n",
        "        return next_hunter_state1, next_hunter_state2, next_hunter_state3, next_prey_state, t\n",
        "    \n",
        "    def update(self):\n",
        "        self.learning_steps += 1\n",
        "        \n",
        "        hunter_states1, actions1, rewards1, dones1, next_hunter_states1 = self.buffer1.sample(self.batch_size)\n",
        "        hunter_states2, actions2, rewards2, dones2, next_hunter_states2 = self.buffer2.sample(self.batch_size)\n",
        "        hunter_states3, actions3, rewards3, dones3, next_hunter_states3 = self.buffer3.sample(self.batch_size)\n",
        "        \n",
        "        self.update_critic(hunter_states1, actions1, rewards1, dones1, next_hunter_states1, self.critic1, self.actor1, self.critic_target1, self.optim_critic1)\n",
        "        self.update_critic(hunter_states2, actions2, rewards2, dones2, next_hunter_states2, self.critic2, self.actor2, self.critic_target2, self.optim_critic2)\n",
        "        self.update_critic(hunter_states3, actions3, rewards3, dones3, next_hunter_states3, self.critic3, self.actor3, self.critic_target3, self.optim_critic3)\n",
        "\n",
        "        self.update_actor(hunter_states1, self.critic1, self.actor1, self.optim_actor1)\n",
        "        self.update_actor(hunter_states2, self.critic2, self.actor2, self.optim_actor2)\n",
        "        self.update_actor(hunter_states3, self.critic3, self.actor3, self.optim_actor3)\n",
        "        self.update_target()\n",
        "        \n",
        "    def update_critic(self, states, actions, rewards, dones, next_states, critic, actor, critic_target, optim_critic):\n",
        "        curr_qs1, curr_qs2 = critic(states, actions)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            next_actions, log_pis = actor.sample(next_states)\n",
        "            next_qs1, next_qs2 = critic_target(next_states, next_actions)\n",
        "            next_qs = torch.min(next_qs1, next_qs2) - self.alpha * log_pis\n",
        "        target_qs = rewards * self.reward_scale + (1.0 - dones) * self.gamma * next_qs\n",
        "        \n",
        "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
        "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
        "        \n",
        "        optim_critic.zero_grad()\n",
        "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
        "        optim_critic.step()\n",
        "    \n",
        "    def update_actor(self, states, critic, actor, optim_actor):\n",
        "        actions, log_pis = actor.sample(states)\n",
        "        qs1, qs2 = critic(states, actions)\n",
        "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
        "        \n",
        "        optim_actor.zero_grad()\n",
        "        loss_actor.backward(retain_graph=False)\n",
        "        optim_actor.step()\n",
        "        \n",
        "    def update_target(self):\n",
        "        for t, s in zip(self.critic_target1.parameters(), self.critic1.parameters()):\n",
        "            t.data.mul_(1.0 - self.tau)\n",
        "            t.data.add_(self.tau * s.data)\n",
        "        \n",
        "        for t, s in zip(self.critic_target2.parameters(), self.critic2.parameters()):\n",
        "            t.data.mul_(1.0 - self.tau)\n",
        "            t.data.add_(self.tau * s.data)\n",
        "        \n",
        "        for t, s in zip(self.critic_target3.parameters(), self.critic3.parameters()):\n",
        "            t.data.mul_(1.0 - self.tau)\n",
        "            t.data.add_(self.tau * s.data)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmhkdTZijPGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, env, env_test, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
        "        self.env = env\n",
        "        self.env_test = env_test\n",
        "        self.algo = algo\n",
        "        \n",
        "        # self.env.seed(seed)\n",
        "        # self.env_test.seed(2**32 - seed)\n",
        "        \n",
        "        self.returns = {'step': [], 'return': []}\n",
        "        \n",
        "        self.num_steps = num_steps\n",
        "        self.eval_interval = eval_interval\n",
        "        self.num_eval_episodes = num_eval_episodes\n",
        "        self.prey_state = env.prey_state\n",
        "        \n",
        "    def train(self):\n",
        "        self.start_time = time()\n",
        "        \n",
        "        t = 0\n",
        "        \n",
        "        hunter_state1, hunter_state2, hunter_state3, prey_state = self.env.initialize_state()\n",
        "        \n",
        "        for steps in range(1, self.num_steps + 1):\n",
        "          hunter_state1, hunter_state2, hunter_state3, prey_state, t = self.algo.step(self.env, hunter_state1, hunter_state2, hunter_state3, t, steps)\n",
        "            \n",
        "          if(self.algo.is_update(steps)):\n",
        "              self.algo.update()\n",
        "                \n",
        "          if(steps % self.eval_interval == 0):\n",
        "              self.evaluate(steps)\n",
        "\n",
        "          if(steps % env.max_episode_steps == 0):\n",
        "              hunter_state1, hunter_state2, hunter_state3, prey_state = self.env.initialize_state()\n",
        "\n",
        "          if(steps % 1000 == 0):\n",
        "              print(steps)\n",
        "              print(prey_state)\n",
        "                \n",
        "    def evaluate(self, steps):\n",
        "        returns = []\n",
        "        \n",
        "        for _ in range(self.num_eval_episodes):\n",
        "            hunter_state1, hunter_state2, hunter_state3, self.prey_state = self.env_test.initialize_state()\n",
        "            done = False\n",
        "            episode_return = 0.0\n",
        "            s = 0\n",
        "            print('New loop ', done)\n",
        "            \n",
        "            while(not done):\n",
        "                if(s == env.max_episode_steps):\n",
        "                  break\n",
        "                s += 1\n",
        "                action_hunter1 = self.algo.exploit(hunter_state1, self.algo.actor1)\n",
        "                action_hunter2 = self.algo.exploit(hunter_state2, self.algo.actor2)\n",
        "                action_hunter3 = self.algo.exploit(hunter_state3, self.algo.actor3)\n",
        "                hunter_state1, hunter_state2, hunter_state3, prey_state, reward, done = self.env_test.step(action_hunter1, action_hunter2, action_hunter3)\n",
        "                episode_return += reward\n",
        "                print(reward, done, hunter_state1, hunter_state2, hunter_state3, prey_state)\n",
        "            \n",
        "            returns.append(episode_return)\n",
        "        \n",
        "        mean_return = np.mean(returns)\n",
        "        self.returns['step'].append(steps)\n",
        "        self.returns['return'].append(mean_return)\n",
        "        \n",
        "        print(f'Num steps: {steps:<6} '\n",
        "             f'Return: {mean_return:<5.1f} '\n",
        "             f'Time: {self.time}')\n",
        "        \n",
        "    def visualize(self):\n",
        "        return\n",
        "    \n",
        "    def plot(self):\n",
        "        fig = plt.figure(figsize=(8,6))\n",
        "        plt.plot(self.returns['step'], self.returns['return'])\n",
        "        plt.xlabel('Steps', fontsize=24)\n",
        "        plt.ylabel('Return', fontsize24)\n",
        "        plt.tick_params(labelsize=18)\n",
        "        plt.tight_layout()\n",
        "\n",
        "    def time(self):\n",
        "      return str(timedelta(seconds=int(time() - self.start_time)))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYH3FfXZjU-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 0\n",
        "REWARD_SCALE = 5.0\n",
        "NUM_STEPS = 10 ** 7\n",
        "EVAL_INTERVAL = 10 ** 4\n",
        "\n",
        "algo = SAC(\n",
        "    state_shape=(2,),\n",
        "    action_shape=(2,),\n",
        "    # seed=SEED,\n",
        "    reward_scale=REWARD_SCALE\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    env=Environment(),\n",
        "    env_test=Environment(),\n",
        "    algo=algo,\n",
        "    # seed=SEED,\n",
        "    num_steps=NUM_STEPS,\n",
        "    eval_interval=EVAL_INTERVAL\n",
        ")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKa0jJ0qjYUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3293c926-08b0-4811-95bb-72a58de5d9bc"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "[0.61459379 0.50514041]\n",
            "2000\n",
            "[0.03573138 0.03654005]\n",
            "3000\n",
            "[0.55539599 0.58235771]\n",
            "4000\n",
            "[0.38126914 0.1855671 ]\n",
            "5000\n",
            "[0.16071907 0.29694245]\n",
            "6000\n",
            "[0.00328134 0.44954318]\n",
            "7000\n",
            "[0.26959508 0.34348858]\n",
            "8000\n",
            "[0.44377657 0.89358   ]\n",
            "9000\n",
            "[0.353671   0.76579535]\n",
            "New loop  False\n",
            "-0.2 False [-0.05872571  1.21627746] [0.25634118 0.39815086] [0.16930184 0.30427325] [0.08894684 0.84768977]\n",
            "-0.2 False [-0.21504499  1.69121355] [0.28916709 0.43097677] [0.16565641 0.30062781] [0.10514716 0.86389009]\n",
            "-0.2 False [-0.37873757  2.16365903] [0.33280391 0.47461359] [0.16310609 0.2980775 ] [0.12119707 0.87994   ]\n",
            "-0.2 False [-0.54908737  2.63374514] [0.38650847 0.52831815] [0.16217587 0.29714728] [0.13714021 0.89588314]\n",
            "-0.2 False [-0.72568582  3.10151964] [0.44899662 0.5908063 ] [0.16370346 0.29867487] [0.15304354 0.91178647]\n",
            "-0.2 False [-0.90824754  3.5669992 ] [0.52129903 0.66310871] [0.16761888 0.30259029] [0.16901227 0.9277552 ]\n",
            "-0.2 False [-1.09664405  4.03014794] [0.60334796 0.74515763] [0.17361559 0.308587  ] [0.18514278 0.94388571]\n",
            "-0.2 False [-1.29076689  4.4909259 ] [0.69598197 0.83779165] [0.18141571 0.31638712] [0.20153323 0.96027616]\n",
            "-0.2 False [-1.49056529  4.9492714 ] [0.7997719  0.94158158] [0.18944283 0.32441424] [0.21824468 0.97698761]\n",
            "-0.2 False [-1.69595758  5.40513761] [0.91364873 1.05545841] [0.19577828 0.33074969] [0.23523816 0.99398109]\n",
            "New loop  False\n",
            "-0.2 False [-0.43812343  0.61819786] [0.62626062 0.13431584] [0.3251887  0.83170117] [0.3636618  0.61023364]\n",
            "-0.2 False [-0.93534545  0.67083114] [0.63523936 0.14329458] [0.32477942 0.83129189] [0.39192353 0.63849537]\n",
            "-0.2 False [-1.43305445  0.7186406 ] [0.62704921 0.13510444] [0.32459398 0.83110646] [0.420177   0.66674884]\n",
            "-0.2 False [-1.93094192  0.76455438] [0.61287798 0.1209332 ] [0.32420435 0.83071683] [0.44842672 0.69499856]\n",
            "-0.2 False [-2.42892316  0.80943959] [0.59269721 0.10075244] [0.3235007  0.83001317] [0.47666857 0.72324041]\n",
            "-0.2 False [-2.92694534  0.85386829] [0.56464413 0.07269936] [0.32245105 0.82896353] [0.50489623 0.75146807]\n",
            "-0.2 False [-3.42496338  0.89834328] [0.53091834 0.03897357] [0.32094491 0.82745739] [0.53310268 0.77967452]\n",
            "-0.2 False [-3.92295237  0.94314253] [0.49270995 0.00076518] [0.31891827 0.82543075] [0.5612787  0.80785054]\n",
            "-0.2 False [-4.42089666  0.98843578] [ 0.45026116 -0.04168362] [0.31656862 0.8230811 ] [0.5894152  0.83598704]\n",
            "-0.2 False [-4.91878473  1.034343  ] [ 0.4047141  -0.08723068] [0.31405649 0.82056897] [0.61750829 0.86408013]\n",
            "New loop  False\n",
            "-0.2 False [1.1435566  1.05310506] [0.05887353 0.35832924] [0.67085387 0.41475751] [0.16444263 0.84218067]\n",
            "-0.2 False [1.57064904 1.31308977] [0.12455859 0.4240143 ] [0.66229201 0.40619565] [0.18930933 0.86704737]\n",
            "-0.2 False [1.99936818 1.57038318] [0.20305549 0.5025112 ] [0.65586788 0.39977152] [0.21395427 0.8916923 ]\n",
            "-0.2 False [2.42957058 1.82518879] [0.29517627 0.59463198] [0.652734   0.39663764] [0.23841017 0.91614821]\n",
            "-0.2 False [2.86120058 2.07756854] [0.4025571  0.70201281] [0.65307801 0.39698165] [0.26276954 0.94050758]\n",
            "-0.2 False [3.29424838 2.32750774] [0.52634086 0.82579657] [0.65870722 0.40261086] [0.2871395  0.96487754]\n",
            "-0.2 False [3.7287432  2.57492288] [0.66498568 0.9644414 ] [0.66761209 0.41151574] [0.31168123 0.98941927]\n",
            "-0.2 False [4.16468618 2.81977736] [0.81631022 1.11576593] [0.67636775 0.42027139] [0.33646446 1.0142025 ]\n",
            "-0.2 False [4.60204067 3.06210163] [0.98086978 1.28032549] [0.68226227 0.42616591] [0.36146468 1.03920272]\n",
            "-0.2 False [5.04076403 3.30193869] [1.15904095 1.45849666] [0.6845018  0.42840544] [0.3865993  1.06433734]\n",
            "Num steps: 10000  Return: -2.0  Time: <bound method Trainer.time of <__main__.Trainer object at 0x7f5b8ede1710>>\n",
            "10000\n",
            "[0.4703292  0.49731571]\n",
            "11000\n",
            "[0.34395741 0.70437092]\n",
            "12000\n",
            "[0.070381   0.30705424]\n",
            "13000\n",
            "[0.92654206 0.74363687]\n",
            "14000\n",
            "[0.59474752 0.41738695]\n",
            "15000\n",
            "[0.3258869  0.66017869]\n",
            "16000\n",
            "[0.04398927 0.98930958]\n",
            "17000\n",
            "[0.70051758 0.49993736]\n",
            "18000\n",
            "[0.53535125 0.45170172]\n",
            "19000\n",
            "[0.02286799 0.02232837]\n",
            "New loop  False\n",
            "-0.2 False [-0.41371746  0.91238672] [0.35811958 0.52113559] [0.80932132 0.81360657] [0.88463041 0.51368398]\n",
            "-0.2 False [-0.89644809  1.04266003] [0.52232017 0.68533618] [0.80983055 0.8141158 ] [0.9150176  0.54407117]\n",
            "-0.2 False [-1.38480302  1.14994223] [0.63996526 0.80298127] [0.81621627 0.82050152] [0.94541556 0.57446913]\n",
            "-0.2 False [-1.8759294   1.24372321] [0.68119913 0.84421514] [0.8251259  0.82941115] [0.97594795 0.60500152]\n",
            "-0.2 False [-2.36809779  1.33187238] [0.63547068 0.79848669] [0.8334309  0.83771615] [1.00661674 0.63567031]\n",
            "-0.2 False [-2.86019502  1.42041793] [0.51135218 0.67436819] [0.84231087 0.84659612] [1.03739714 0.66645071]\n",
            "-0.2 False [-3.3514955   1.51328258] [0.31386312 0.47687913] [0.84846196 0.85274721] [1.06829206 0.69734563]\n",
            "-0.2 False [-3.84135286  1.61348122] [0.05869367 0.22170967] [0.84706494 0.85135019] [1.09926361 0.72831718]\n",
            "-0.2 False [-4.32919614  1.72306651] [-0.24512981 -0.0821138 ] [0.83698627 0.84127153] [1.13021795 0.75927152]\n",
            "-0.2 False [-4.81447352  1.84350681] [-0.58877901 -0.425763  ] [0.82288265 0.8271679 ] [1.16104422 0.79009779]\n",
            "New loop  False\n",
            "-0.2 False [0.29312692 1.05444508] [0.33588189 0.31994168] [0.52209666 0.25852142] [0.92301514 0.68426746]\n",
            "-0.2 False [0.07882628 1.50619194] [0.24627757 0.23033737] [0.51946469 0.25588945] [0.94332663 0.70457896]\n",
            "-0.2 False [-0.12350622  1.96342445] [-0.04281826 -0.05875846] [0.51450619 0.25093096] [0.96356512 0.72481745]\n",
            "-0.2 False [-0.2941528  2.4334029] [-0.45196071 -0.46790091] [0.50294111 0.23936588] [0.98365819 0.74491052]\n",
            "-0.2 False [-0.43012649  2.91455896] [-0.91246698 -0.92840718] [0.49831504 0.23473981] [1.003371   0.76462333]\n",
            "-0.2 False [-0.53603056  3.40321459] [-1.39014927 -1.40608947] [0.50761688 0.24404165] [1.02292913 0.78418146]\n",
            "-0.2 False [-0.61750022  3.89653264] [-1.87316935 -1.88910955] [0.52783773 0.2642625 ] [1.04279815 0.80405048]\n",
            "-0.2 False [-0.67897736  4.39273881] [-2.35779575 -2.37373596] [0.55278762 0.28921238] [1.06326258 0.82451491]\n",
            "-0.2 False [-0.72365067  4.8907391 ] [-2.84288201 -2.85882222] [0.58210693 0.3185317 ] [1.08426709 0.84551941]\n",
            "-0.2 False [-0.75397431  5.38981873] [-3.32810332 -3.34404352] [0.61454562 0.35097039] [1.10611598 0.86736831]\n",
            "New loop  False\n",
            "-0.2 False [ 0.59588131 -0.18975616] [0.74651883 0.67801082] [0.08093486 0.47779381] [0.17480202 0.8276326 ]\n",
            "-0.2 False [ 0.74291403 -0.66764881] [0.91876421 0.8502562 ] [0.08719787 0.48405682] [0.19465714 0.84748772]\n",
            "-0.2 False [ 0.86220816 -1.15320922] [1.21566802 1.14716002] [0.09192999 0.48878894] [0.21479708 0.86762766]\n",
            "-0.2 False [ 0.9500532  -1.64543198] [1.60944762 1.54093961] [0.08777756 0.48463651] [0.23515081 0.88798139]\n",
            "-0.2 False [ 1.00811634 -2.14204921] [2.05380485 1.98529685] [0.07378651 0.47064546] [0.25531695 0.90814754]\n",
            "-0.2 False [ 1.04080546 -2.64097949] [2.52032894 2.45182094] [0.05160955 0.4484685 ] [0.27494811 0.92777869]\n",
            "-0.2 False [ 1.0525259 -3.1408421] [2.9968995 2.9283915] [0.02236674 0.41922569] [0.29383677 0.94666735]\n",
            "-0.2 False [ 1.04685712 -3.64080996] [3.47819039 3.40968238] [-0.0123778   0.38448115] [0.31168187 0.96451245]\n",
            "-0.2 False [ 1.02656523 -4.14039803] [3.96155826 3.89305025] [-0.05254576  0.34431319] [0.32814349 0.98097407]\n",
            "-0.2 False [ 0.99382694 -4.63932508] [4.44586366 4.37735565] [-0.09679576  0.30006319] [0.34296856 0.99579914]\n",
            "Num steps: 20000  Return: -2.0  Time: <bound method Trainer.time of <__main__.Trainer object at 0x7f5b8ede1710>>\n",
            "20000\n",
            "[0.2031548  0.20990664]\n",
            "21000\n",
            "[0.66952051 0.80106221]\n",
            "22000\n",
            "[0.54467049 0.18666673]\n",
            "23000\n",
            "[0.14921157 0.91192595]\n",
            "24000\n",
            "[0.3259698  0.71622273]\n",
            "25000\n",
            "[0.76066181 0.95279258]\n",
            "26000\n",
            "[0.60243851 0.57386472]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riN9Ok9rOdX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}